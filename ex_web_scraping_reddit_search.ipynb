{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition: Reddit Scraping\n",
    "\n",
    "In this exercise, we will search a query (e.g., \"data science\") on the old Reddit interface (https://www.old.reddit.com/). We will then grab the url (e.g., https://old.reddit.com/search?q=data+science) of the search page and scrap the returned posts. The reason for using the old Reddit interface is that the html tags are user-friendly. We will focus on extracting title, author, author's profile, subreddit, tag, timestamp, number of votes, and number of comments. \n",
    "<img src=\"../images/reddit_search.png\" />\n",
    "\n",
    "\n",
    "\n",
    "* You are free to use your own query string. \n",
    "* On the search page, a set of subreddits are shown. Ignore these subreddits and focus on extracting Reddit posts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 1:** Fetch the page and create a soup object using Beautiful soup library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for activity 1 goes here..\n",
    "#---------------------------------------\n",
    "\n",
    "#import the library to query a website\n",
    "import requests\n",
    "# import Beautiful soup library to access \n",
    "# functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import pandas to convert list to data frame\n",
    "import pandas as pd\n",
    "#imprt numpy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'MyAPP/1.0'}  \n",
    "# this will make sure our query is comming from a browser and it's not a bot\n",
    "\n",
    "\n",
    "# specify the url\n",
    "url = \"https://old.reddit.com/search?q=data+science\"\n",
    "# Open website URL and return the html to the variable 'response'\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the html in the 'response' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(response.text,\"html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Extract the titles and URLs of the retrieved posts from the soup and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"search-result search-result-subreddit\" data-fullname=\"t5_2sptq\"><header class=\"search-result-header\"><a class=\"search-title may-blank\" href=\"https://old.reddit.com/r/datascience/\">Data Science</a></header><div class=\"search-result-meta\"><span class=\"fancy-toggle-button search-subscribe-button toggle\" data-sr_name=\"datascience\" style=\"\"><a class=\"option active add login-required\" href=\"#\" tabindex=\"100\">join</a><a class=\"option remove\" href=\"#\">leave</a></span><a class=\"search-subreddit-link may-blank\" href=\"https://old.reddit.com/r/datascience/\">r/datascience</a> <span class=\"search-subscribers\">2,141,035 members,</span> <span class=\"search-time\">a community for 13 years</span> </div><div class=\"search-result-body\">A space for data science professionals to engage in discussions and debates on the subject of data science.</div><div class=\"search-result-footer\"><span class=\"search-result-icon search-result-icon-filter\"></span><a class=\"search-link\" href=\"https://old.reddit.com/r/datascience/search?q=data+science&amp;restrict_sr=on\" title=\"search in r/datascience\">search within r/datascience</a></div></div>, <div class=\"search-result search-result-subreddit\" data-fullname=\"t5_2xc7p\"><header class=\"search-result-header\"><a class=\"search-title may-blank\" href=\"https://old.reddit.com/r/DataScienceJobs/\">Data Science &amp; Machine Learning Jobs!</a></header><div class=\"search-result-meta\"><span class=\"fancy-toggle-button search-subscribe-button toggle\" data-sr_name=\"DataScienceJobs\" style=\"\"><a class=\"option active add login-required\" href=\"#\" tabindex=\"100\">join</a><a class=\"option remove\" href=\"#\">leave</a></span><span class=\"stamp restricted-stamp\">restricted</span> <a class=\"search-subreddit-link may-blank\" href=\"https://old.reddit.com/r/DataScienceJobs/\">r/DataScienceJobs</a> <span class=\"search-subscribers\">28,244 members,</span> <span class=\"search-time\">a community for 11 years</span> </div><div class=\"search-result-body\">A place for people to post data science/machine learning jobs as well as those searching for jobs to put themselves in the spotlight.</div><div class=\"search-result-footer\"><span class=\"search-result-icon search-result-icon-filter\"></span><a class=\"search-link\" href=\"https://old.reddit.com/r/DataScienceJobs/search?q=data+science&amp;restrict_sr=on\" title=\"search in r/DataScienceJobs\">search within r/DataScienceJobs</a></div></div>, <div class=\"search-result search-result-subreddit\" data-fullname=\"t5_3h8vh\"><header class=\"search-result-header\"><a class=\"search-title may-blank\" href=\"https://old.reddit.com/r/DataScienceProjects/\">Data Science Projects</a></header><div class=\"search-result-meta\"><span class=\"fancy-toggle-button search-subscribe-button toggle\" data-sr_name=\"DataScienceProjects\" style=\"\"><a class=\"option active add login-required\" href=\"#\" tabindex=\"100\">join</a><a class=\"option remove\" href=\"#\">leave</a></span><a class=\"search-subreddit-link may-blank\" href=\"https://old.reddit.com/r/DataScienceProjects/\">r/DataScienceProjects</a> <span class=\"search-subscribers\">4,173 members,</span> <span class=\"search-time\">a community for 7 years</span> </div><div class=\"search-result-body\">A subreddit for sharing progress on data science projects or for seeking collaborators on projects.</div><div class=\"search-result-footer\"><span class=\"search-result-icon search-result-icon-filter\"></span><a class=\"search-link\" href=\"https://old.reddit.com/r/DataScienceProjects/search?q=data+science&amp;restrict_sr=on\" title=\"search in r/DataScienceProjects\">search within r/DataScienceProjects</a></div></div>]\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "posts = soup.find_all('div', class_='search-result')\n",
    "print(posts)\n",
    "titles_and_urls = []\n",
    "\n",
    "for post in posts:\n",
    "    title_tag = post.find('h3') \n",
    "    print(title_tag)\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text()\n",
    "        url = post.find('a', href=True)['href']  \n",
    "        titles_and_urls.append((title, url))\n",
    "\n",
    "for title, url in titles_and_urls:\n",
    "    print(f\"Title: {title}, URL: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3:** Extract the author ids and their profile links from the retrieved posts and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = {'class': 'search-result'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "posts = soup.find_all('div',attrs=attrs)\n",
    "\n",
    "authors_and_profiles = []\n",
    "#print(posts)\n",
    "for post in posts:\n",
    "    author_tag = post.find('a', class_='author')  \n",
    "    print(author_tag)\n",
    "    if author_tag:\n",
    "        author_id = author_tag.get_text()\n",
    "        profile_url = author_tag['href'] \n",
    "        authors_and_profiles.append((author_id, profile_url))\n",
    "\n",
    "for author_id, profile_url in authors_and_profiles:\n",
    "    print(f\"Author ID: {author_id}, Profile URL: {profile_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 4:** Extract the submission time of the retrieved posts and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "posts = soup.find_all('div', class_='search-result')\n",
    "\n",
    "submission_times = []\n",
    "\n",
    "for post in posts:\n",
    "    time_tag = post.find('time')  \n",
    "    if time_tag:\n",
    "        submission_time = time_tag['datetime']  \n",
    "        submission_times.append(submission_time)\n",
    "\n",
    "for submission_time in submission_times:\n",
    "    print(f\"Submission Time: {submission_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 5:** Extract the subreddits of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "posts = soup.find_all('div', class_='search-result')\n",
    "\n",
    "subreddits = []\n",
    "\n",
    "for post in posts:\n",
    "    subreddit_tag = post.find('a', class_='subreddit')  \n",
    "    if subreddit_tag:\n",
    "        subreddit_name = subreddit_tag.get_text()  \n",
    "        subreddits.append(subreddit_name)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Subreddit: {subreddit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 6:** Extract the points of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "posts = soup.find_all('div', class_='search-result')\n",
    "\n",
    "points = []\n",
    "\n",
    "for post in posts:\n",
    "    points_tag = post.find('div', class_='score')  \n",
    "    if points_tag:\n",
    "        post_points = points_tag.get_text()  \n",
    "        points.append(post_points)\n",
    "\n",
    "for post_points in points:\n",
    "    print(f\"Points: {post_points}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 7:** Extract the number of comments of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "posts = soup.find_all('div', class_='search-result')\n",
    "\n",
    "comments = []\n",
    "\n",
    "for post in posts:\n",
    "    comments_tag = post.find('a', class_='comments')  \n",
    "    if comments_tag:\n",
    "        post_comments = comments_tag.get_text()  \n",
    "        comments.append(post_comments)\n",
    "\n",
    "for post_comments in comments:\n",
    "    print(f\"Comments: {post_comments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 8:** Using the above features you extracted, create a dataframe for the retrieved posts, and print the first 10 entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Title       URL    Author Subreddit    Points  Comments Submission Time\n",
      "0  Ellipsis  Ellipsis  Ellipsis  Ellipsis  Ellipsis  Ellipsis        Ellipsis\n"
     ]
    }
   ],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "titles = [...]  \n",
    "urls = [...]  \n",
    "authors = [...]  \n",
    "subreddits = [...]  \n",
    "points = [...]  \n",
    "comments = [...] \n",
    "submission_times = [...]  \n",
    "\n",
    "data = {\n",
    "    'Title': titles,\n",
    "    'URL': urls,\n",
    "    'Author': authors,\n",
    "    'Subreddit': subreddits,\n",
    "    'Points': points,\n",
    "    'Comments': comments,\n",
    "    'Submission Time': submission_times\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 9:** Save the retrieved posts in a json file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to reddit_posts.json\n"
     ]
    }
   ],
   "source": [
    "# Your Code Below:\n",
    "# ----------------\n",
    "import json\n",
    "\n",
    "titles = ['Title 1', 'Title 2'] \n",
    "urls = ['https://example.com/post1', 'https://example.com/post2']  \n",
    "authors = ['Author1', 'Author2'] \n",
    "subreddits = ['subreddit1', 'subreddit2']  \n",
    "points = [100, 200]  \n",
    "comments = [10, 20]  \n",
    "submission_times = ['2023-09-18T12:00:00', '2023-09-18T13:00:00'] \n",
    "\n",
    "posts_data = []\n",
    "for i in range(len(titles)):\n",
    "    post = {\n",
    "        'Title': titles[i],\n",
    "        'URL': urls[i],\n",
    "        'Author': authors[i],\n",
    "        'Subreddit': subreddits[i],\n",
    "        'Points': points[i],\n",
    "        'Comments': comments[i],\n",
    "        'Submission Time': submission_times[i]\n",
    "    }\n",
    "    posts_data.append(post)\n",
    "\n",
    "with open('reddit_posts.json', 'w') as json_file:\n",
    "    json.dump(posts_data, json_file, indent=4)\n",
    "\n",
    "print(\"Data saved to reddit_posts.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification:** Let's reload the JSON file and print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Title\": \"Title 1\",\n",
      "        \"URL\": \"https://example.com/post1\",\n",
      "        \"Author\": \"Author1\",\n",
      "        \"Subreddit\": \"subreddit1\",\n",
      "        \"Points\": 100,\n",
      "        \"Comments\": 10,\n",
      "        \"Submission Time\": \"2023-09-18T12:00:00\"\n",
      "    },\n",
      "    {\n",
      "        \"Title\": \"Title 2\",\n",
      "        \"URL\": \"https://example.com/post2\",\n",
      "        \"Author\": \"Author2\",\n",
      "        \"Subreddit\": \"subreddit2\",\n",
      "        \"Points\": 200,\n",
      "        \"Comments\": 20,\n",
      "        \"Submission Time\": \"2023-09-18T13:00:00\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "with open('reddit_posts.json', 'r') as f:\n",
    "    parsed = json.load(f)\n",
    "    print( json.dumps(parsed, indent=4) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
