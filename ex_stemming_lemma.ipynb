{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home*\n",
    "\n",
    "For this exercise, there are 100 sms that have been parsed and categorized as \"Spam\" or \"Ham\". The dataframe also contains the original text message. We have converted the dataframe into a dictionary for this exercise (execute the first two cells).\n",
    "\n",
    "In the given dictionary, there are 100 entries, starting from 0 to 99 as the keys. The value for each of them is two strings, `class` and `text`. `class` contains either \"spam\" or \"ham\", based on the category of the sms, and `text` contains the original text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dsa/data/DSA-8410/spam.csv\", encoding='latin1')\n",
    "mini_df = df[['v1', 'v2']][:100]\n",
    "mini_df.columns = ['class', 'text']\n",
    "\n",
    "mini_df.to_csv('messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages.csv')\n",
    "msgs = df.T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Create a list of strings from this dictionary with the `text` values, and convert all of the strings into lowercase. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "print(msgs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...', 'ok lar... joking wif u oni...', \"free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's\", 'u dun say so early hor... u c already then say...', \"nah i don't think he goes to usf, he lives around here though\"]\n"
     ]
    }
   ],
   "source": [
    "text_list = [msgs[i]['text'].lower() for i in msgs]\n",
    "print(text_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Use `nltk` packages tokenize functionality on each of the strings in your list. The result should be a list of lists. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/djkgg/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', 'c', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...'], ['nah', 'i', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though']]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokenized_list = [word_tokenize(text) for text in text_list]\n",
    "print(tokenized_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.** Remove the stopwords, punctuations and numbers from your list (list of lists). Punctuations and numbers can be checked by the function `string.punctuation` used after a string. If the result is false, you can remove that particular string from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', '..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'got', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'early', 'hor', '...', 'u', 'c', 'already', 'say', '...'], ['nah', \"n't\", 'think', 'goes', 'usf', 'lives', 'around', 'though']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/djkgg/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [word for word in tokens if word not in stop_words and word not in punctuations and not word.isdigit()]\n",
    "\n",
    "cleaned_list = [clean_tokens(tokens) for tokens in tokenized_list]\n",
    "\n",
    "print(cleaned_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.** Use `nltk` packages `PorterStemmer` to stem the cleaned-text list that you got as a result of **Task 3**. Use a new variable to store the stemmed-word list, and keep the result from the **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazi', '..', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'got', 'amor', 'wat', '...'], ['ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...'], ['free', 'entri', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', '21st', 'may', '2005.', 'text', 'fa', 'receiv', 'entri', 'question', 'std', 'txt', 'rate', 'c', \"'s\", 'appli', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'earli', 'hor', '...', 'u', 'c', 'alreadi', 'say', '...'], ['nah', \"n't\", 'think', 'goe', 'usf', 'live', 'around', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_list = [[stemmer.stem(word) for word in tokens] for tokens in cleaned_list]\n",
    "\n",
    "print(stemmed_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.** Use `nltk` packages `WordNetLemmatizer` to find the lemma (or root word) from the cleaned-text list that you got as a result of **Task 3**. Consider all of the words to be a `Verb`. Use a new variable to store the lemmatized-word list, and keep the result from **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks. We assume every word is a verb to make the problem easier, but we could have applied a `POS` tagger and inferred the POS for that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/djkgg/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', '..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'get', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'early', 'hor', '...', 'u', 'c', 'already', 'say', '...'], ['nah', \"n't\", 'think', 'go', 'usf', 'live', 'around', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_list = [[lemmatizer.lemmatize(word, pos='v') for word in tokens] for tokens in cleaned_list]\n",
    "\n",
    "print(lemmatized_list[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.** For each lemma that we got from **Task 5**, calculate how many times they occur in all of the messages. Sort them in descending order by the number of total occurrences, and print out the top ten (10) words and their number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...: 24\n",
      "u: 17\n",
      "call: 15\n",
      "n't: 14\n",
      "'s: 13\n",
      "'m: 13\n",
      "go: 11\n",
      "get: 11\n",
      "free: 10\n",
      "like: 10\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from collections import Counter\n",
    "\n",
    "all_words = [word for tokens in lemmatized_list for word in tokens]\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "for word, count in top_10_words:\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7.** From the result we got from **Task 6**, remove all of the words with a length of 1 and select the top hundred (100) most frequent terms from it. We will use this list of words in our next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...: 24\n",
      "call: 15\n",
      "n't: 14\n",
      "'s: 13\n",
      "'m: 13\n",
      "go: 11\n",
      "get: 11\n",
      "free: 10\n",
      "like: 10\n",
      "'': 9\n",
      "..: 8\n",
      "ok: 8\n",
      "sorry: 8\n",
      "txt: 6\n",
      "already: 6\n",
      "home: 6\n",
      "smile: 6\n",
      "say: 5\n",
      "still: 5\n",
      "want: 5\n",
      "reply: 5\n",
      "yes: 5\n",
      "way: 5\n",
      "ur: 5\n",
      "ha: 5\n",
      "finish: 5\n",
      "know: 5\n",
      "anything: 5\n",
      "please: 5\n",
      "see: 5\n",
      "lt: 5\n",
      "gt: 5\n",
      "back: 4\n",
      "send: 4\n",
      "prize: 4\n",
      "claim: 4\n",
      "mobile: 4\n",
      "time: 4\n",
      "try: 4\n",
      "'ll: 4\n",
      "tell: 4\n",
      "later: 4\n",
      "pain: 4\n",
      "come: 4\n",
      "hi: 4\n",
      "great: 3\n",
      "lar: 3\n",
      "joke: 3\n",
      "text: 3\n",
      "think: 3\n",
      "word: 3\n",
      "even: 3\n",
      "customer: 3\n",
      "na: 3\n",
      "tonight: 3\n",
      "cash: 3\n",
      "make: 3\n",
      "feel: 3\n",
      "miss: 3\n",
      "first: 3\n",
      "lor: 3\n",
      "meet: 3\n",
      "lol: 3\n",
      "catch: 3\n",
      "love: 3\n",
      "wait: 3\n",
      "yeah: 3\n",
      "look: 3\n",
      "do: 3\n",
      "need: 3\n",
      "sms: 3\n",
      "man: 3\n",
      "end: 3\n",
      "check: 3\n",
      "wat: 2\n",
      "wif: 2\n",
      "entry: 2\n",
      "win: 2\n",
      "fa: 2\n",
      "may: 2\n",
      "std: 2\n",
      "apply: 2\n",
      "early: 2\n",
      "live: 2\n",
      "around: 2\n",
      "though: 2\n",
      "week: 2\n",
      "'d: 2\n",
      "å£1.50: 2\n",
      "treat: 2\n",
      "request: 2\n",
      "callertune: 2\n",
      "value: 2\n",
      "network: 2\n",
      "months: 2\n",
      "update: 2\n",
      "gon: 2\n",
      "stuff: 2\n",
      "anymore: 2\n",
      "'ve: 2\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "filtered_word_counts = {word: count for word, count in word_counts.items() if len(word) > 1}\n",
    "\n",
    "top_100_words = Counter(filtered_word_counts).most_common(100)\n",
    "\n",
    "for word, count in top_100_words:\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.** For each message (use the lemma-list we created for **Task 5**), calculate the number of times each word from **Task 7** (top-100 words) occurs in that message. \n",
    "Create a **Data-Matrix** using your calculations. Each row should correspond to a message, and each column should correspond to a word from the list we got in **Task 7**. Each cell should correspond to how many times that particular word (from column) occurs for that specific message (from row).\n",
    "\n",
    "You can use Pandas-DataFrame to store your **Data-Matrix**. Print the first five rows of the Data-Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...  call  n't  's  'm  go  get  free  like  ''  ...  request  callertune  \\\n",
      "0    2     0    0   0   0   1    1     0     0   0  ...        0           0   \n",
      "1    2     0    0   0   0   0    0     0     0   0  ...        0           0   \n",
      "2    0     0    0   2   0   0    0     1     0   0  ...        0           0   \n",
      "3    2     0    0   0   0   0    0     0     0   0  ...        0           0   \n",
      "4    0     0    1   0   0   1    0     0     0   0  ...        0           0   \n",
      "\n",
      "   value  network  months  update  gon  stuff  anymore  've  \n",
      "0      0        0       0       0    0      0        0    0  \n",
      "1      0        0       0       0    0      0        0    0  \n",
      "2      0        0       0       0    0      0        0    0  \n",
      "3      0        0       0       0    0      0        0    0  \n",
      "4      0        0       0       0    0      0        0    0  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "import pandas as pd\n",
    "\n",
    "top_100_words = [word for word, count in top_100_words]\n",
    "\n",
    "data_matrix = []\n",
    "\n",
    "for tokens in lemmatized_list:\n",
    "   \n",
    "    word_count = {word: tokens.count(word) for word in top_100_words}\n",
    "    data_matrix.append(word_count)\n",
    "\n",
    "df = pd.DataFrame(data_matrix, columns=top_100_words)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
